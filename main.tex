\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{scalerel}

% If you want to use named tensor notation in your papers,
% these macros may be useful. Requires \usepackage{amssymb}.
\newcommand{\name}[1]{\mathsf{#1}}
\newcommand{\ndot}[1]{\mathbin{\mathop{\thinspace\boldsymbol\cdot\thinspace}\displaylimits_{\name{#1}}}}
\newcommand{\nsum}[1]{\mathop{\sum}_{\name{#1}}}
\newcommand{\nconcat}[1]{\mathop{\oplus}_{\name{#1}}}
\newcommand{\nfun}[2]{\mathop{\text{#2}}\displaylimits_{\name{#1}}}

% "experimental" notation
\newcommand{\nndot}[2]{\mathbin{\mathop{\thinspace\boldsymbol\cdot\thinspace}\displaylimits_{\name{#1}|\name{#2}}}}

% for formal definitions
\newcommand{\tuple}[1]{\{ #1\}}
\DeclareMathOperator{\tupledom}{dom}
\DeclareMathOperator{\tupleshape}{ind}
\newcommand{\tupleproj}[2]{#1.#2}
\newcommand{\tuplerestrict}[2]{\left.#1\right|_{#2}}
\newcommand{\nmatrix}[3]{\name{#1}\begin{array}[b]{@{}c@{}}\name{#2}\\\begin{bmatrix}#3\end{bmatrix}\end{array}}

\DeclareMathOperator*{\softmax}{softmax}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\dmodel}{d_{\text{model}}}

\DeclareMathOperator*{\bigbowtie}{\scalerel*{\bowtie}{\textstyle\sum}}

\usepackage{parskip}
\setcounter{tocdepth}{1}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}

\title{Named Tensor Notation}
\author{David Chiang \quad Sasha Rush \quad Tongfei Chen \quad Chu-Cheng Lin}

\begin{document}

\maketitle

\tableofcontents

\section{Introduction}

Most papers about neural networks use the notation of vectors and matrices from applied linear algebra. This notation is very well-suited to talking about vector spaces, but less well-suited to talking about neural networks. Consider the following equation \citep{vaswani+:2017}:
\[ \text{Attention}(Q, K, V) = \softmax \left( \frac{QK^\top}{\sqrt{d_k}} \right) V. \]
where $Q$, $K$, and $V$ are sequences of query, key, and value vectors packed into matrices. Does the product $QK^\top$ sum over the sequence, or over the query/key features? We would need to know the sizes of $Q$, $K$, and $V$ to know that it's taken over the query/key features. Is the softmax taken over the query sequence or the key sequence? The usual notation doesn't even offer a way to answer this question. With multiple attention heads, the notation becomes more complicated and leaves more questions unanswered. With multiple sentences in a minibatch, the notation becomes more complicated still, and most papers wisely leave this detail out.

Libraries for programming with neural networks \citep{numpy,pytorch} provide multidimensional arrays, called tensors (although usually without the theory associated with tensors in linear algebra and physics), and a rich array of operations on tensors. But they inherit from math the convention of identifying indices by \emph{position}, making code bug-prone. Quite a few libraries have been developed to identify indices by \emph{name} instead: Nexus \citep{chen2017typesafe}, tsalib \citep{tsalib}, NamedTensor \citep{namedtensor}, named tensors in PyTorch \citep{named-tensors}, and Dex \citep{maclaurin+:2019}. (Some of these libraries also add types to indices, but here we are only interested in adding names.)

Back in the realm of mathematical notation, then, we want two things: first, the flexibility of working with multidimensional arrays, and second, the perspicuity of identifying indices by name instead of by position. This document describes our proposal to do both.

As a preview, the above equation becomes
\begin{equation*}
  \text{Attention}(Q,K,V) = \nfun{time}{softmax} \left( \frac{Q \ndot{key} K}{\sqrt{d_k}} \right) \ndot{time} V
\end{equation*}
making it unambiguous which index each operation applies to. The same equation works with multiple heads and with minibatching.

More examples of the notation are given in \S~\ref{sec:examples}.

The source code for this document can be found at \url{https://github.com/namedtensor/notation/}. We invite anyone to make comments on this proposal by submitting issues or pull requests on this repository.

\section{Informal Overview}
\label{sec:intro}

Let's think first about the usual notions of vectors, matrices, and tensors, without named indices.

Define $[n] = \{1, \ldots, n\}$. We can think of a size-$n$ real vector $v$ as a function from $[n]$ to $\mathbb{R}$. We get the $i$th element of $v$ by applying $v$ to $i$, but we normally write this as $v_i$ (instead of $v(i)$). 

Similarly, we can think of an $m \times n$ real matrix as a function from $[m] \times [n]$ to $\mathbb{R}$, and an $l \times m \times n$ real tensor as a function from $[l] \times [m] \times [n]$ to $\mathbb{R}$. In general, then, real tensors are functions from \emph{tuples of natural numbers} to reals.

\subsection{Named tensors}

A \emph{named tuple} (also known as a \emph{record}) looks like this: \[\tuple{\name{foo}: 2, \name{bar}: 3}.\] The order of the elements doesn't matter: \[\tuple{\name{foo}: 2, \name{bar}: 3} = \tuple{\name{bar}: 3, \name{foo}: 2}.\] We use \textsf{sans-serif} font for names.

Then, a real \emph{named tensor} is a function from named tuples to reals. Each of its indices has a name, and the ordering of the indices doesn't matter. For example, here is a tensor with an index named $\name{foo}$ ranging from 1 to 2 and an index named $\name{bar}$ ranging from 1 to 3. More succinctly, we say that the \emph{shape} of $A$ is $\tuple{\name{foo}:2, \name{bar}:3}$.
\begin{equation*}
A = \nmatrix{foo}{bar}{
  3 & 1 & 4 \\
  1 & 5 & 9
}.
\end{equation*}

We use uppercase italic letters for variables standing for named tensors. We don't mind if you use another convention, but urge you not to use different styles for tensors and their elements. For example, if $\mathbf{A}$ is a tensor, then an element of $\mathbf{A}$ is written as $\mathbf{A}_{\name{foo}:2, \name{bar}:3}$ -- 
not $A_{\name{foo}:2,\name{bar}:3}$ or $a_{\name{foo}:2,\name{bar}:3}$.

Just as the set of all size-$n$ real vectors is written $\mathbb{R}^n$, and the set of all $m\times n$ real matrices is often written $\mathbb{R}^{m \times n}$ (which makes sense because one sometimes writes $Y^X$ for the set of all functions from $X$ to $Y$), we write $\mathbb{R}^{\name{foo}: 2, \name{bar}: 3}$ for the set of all tensors with shape $\tuple{\name{foo}:2, \name{bar}:3}$.

We access elements of $A$ using subscripts: $A_{\name{foo}: 1, \name{bar}: 3} = A_{\name{bar}: 3, \name{foo}: 1} = 4$.
We also allow partial indexing:
\begin{align*}
A_{\name{foo}:1} &= \nmatrix{}{bar}{
  3 & 1 & 4
}
\\
A_{\name{bar}:3} &= \nmatrix{}{foo}{
  4 & 9
}.
\end{align*}

\subsection{Named tensor operations}
\label{sec:operations}

\subsubsection{Elementwise operations}

Any function from scalars to scalars can be applied elementwise to a named tensor:
\begin{equation*}
\exp A = \nmatrix{foo}{bar}{
  \exp 3 & \exp 1 & \exp 4 \\
  \exp 1 & \exp 5 & \exp 9
}.
\end{equation*}
More elementwise unary operations:
\[\begin{array}{cl}
kA & \text{scalar multiplication by $k$} \\
-A & \text{negation} \\
\exp A & \text{elementwise exponential function} \\
\tanh A & \text{hyperbolic tangent} \\
\sigma(A) & \text{logistic sigmoid} \\
\text{ReLU}(A) & \text{rectified linear unit}
\end{array}\]

Any function or operator that takes two scalar arguments can be applied elementwise to two named tensors with the same shape. If $A$ is as above and
\begin{equation*}
B = \nmatrix{foo}{bar}{
  2 & 7 & 1 \\
  8 & 2 & 8
}
\end{equation*}
then
\begin{equation*}
A + B = \nmatrix{foo}{bar}{
  3+2 & 1+7 & 4+1 \\
  1+8 & 5+2 & 9+8
}.
\end{equation*}

But things get more complicated when $A$ and $B$ don't have the same shape. If $A$ and $B$ each have an index with the same name (and size), the two indices are \emph{aligned}, as above. But if $A$ has an index named $\name{i}$ and $B$ doesn't, then we do \emph{broadcasting}, which means effectively that we replace $B$ with a new tensor $B'$ that contains a copy of $B$ for every value of index $\name{i}$.
\begin{align*}
A + 1 &= \nmatrix{foo}{bar}{
  3+1 & 1+1 & 4+1 \\
  1+1 & 5+1 & 9+1
} \\
A + B_{\name{foo:1}} &= \nmatrix{foo}{bar}{
  3+2 & 1+7 & 4+1 \\
  1+2 & 5+7 & 9+1
} \\
A + B_{\name{bar:3}} &= \nmatrix{foo}{bar}{
  3+1 & 1+1 & 4+1 \\
  1+8 & 5+8 & 9+8
}.
\end{align*}
Similarly, if $B$ has an index named $\name{i}$ and $A$ doesn't, then we effectively replace $A$ with a new tensor $A'$ that contains a copy of $A$ for every value of index $\name{i}$. If you've programmed with NumPy or any of its derivatives, this should be unsurprising to you.

More elementwise binary operations:
\[\begin{array}{cl}
A+B & \text{addition} \\
A-B & \text{subtraction} \\
A\odot B & \text{elementwise (Hadamard) product} \\
A / B & \text{elementwise division} \\
\max \{A, B\} & \text{elementwise maximum} \\
\min \{A, B\} & \text{elementwise minimum}
\end{array}\]

\subsubsection{Reductions}

The same rules for alignment and broadcasting apply to functions that take tensor as arguments or return tensors. The gory details are in \S\ref{sec:tensorfunctions}, but we present the most important subcases here. The first is \emph{reductions}, which are functions from vectors to scalars. Unlike with functions on scalars, we always have to specify which index these functions apply to, using a subscript. (This is equivalent to the \verb|axis| argument in NumPy and \verb|dim| in PyTorch.)

For example, using the same example tensor $A$ from above,
\begin{align*}
\nsum{foo} A &= \nmatrix{}{bar}{
  3+1 & 1+5 & 4+9
} \\
\nsum{bar} A &= \nmatrix{}{foo}{
  3+1+4 & 1+5+9
}.
\end{align*}
More reductions: If $A$ has shape $\tuple{\name{i}:X, \ldots}$, then
\begin{align*}
  \nsum{i} A &= \sum_{x \in X} A_{\name{i}: x} \\
  \nfun{i}{norm} A &= \sqrt{\nsum{i} A^2} \\
  \nfun{i}{min} A &= \min_{x \in X} A_{\name{i}: x} \\
  \nfun{i}{max} A &= \max_{x \in X} A_{\name{i}: x} \\
  \nfun{i}{mean} A &= \frac{A}{\nsum{i} A} \\
  \nfun{i}{var} A &= \nsum{i} (A - \nfun{i}{mean} A)^2
\end{align*}
(Note that $\max$ and $\min$ are overloaded; with multiple arguments and no subscript, they are elementwise, and with a single argument and a subscript, they are reductions.)

You can also write multiple names to perform the reduction over multiple indices at once.

\subsubsection{Contraction}

The vector dot product (inner product) is a function from \emph{two} vectors to a scalar, which generalizes to named tensors to give the ubiquitous \emph{contraction} operator:
\begin{align*}
  A \ndot{i} B &= \nsum{i} A \odot B.
\end{align*}
This operator can also be used for matrix-vector or matrix-matrix multiplication:
\begin{align*}
C &= \nmatrix{bar}{baz}{
  1 & -1 \\ 2 & -2 \\ 3 & -3
} \\
A \ndot{bar} C &= \nmatrix{foo}{baz}{
  17 & -17 \\
  53 & -53
}
\end{align*}
However, note that (like vector dot-product, but unlike matrix multiplication) $\ndot{i}$ is commutative, but not associative! Specifically, if
\begin{align*}
A &\in \mathbb{R}^{\name{i}:m} \\
B &\in \mathbb{R}^{\name{i}:m,\name{j}:n} \\
C &\in \mathbb{R}^{\name{i}:m,\name{j}:n}
\end{align*}
then $(A \ndot{i} B) \ndot{j} C$ and $A \ndot{i} (B \ndot{j} C)$ don't even have the same shape.

\subsubsection{Vectors to vectors}

A very common example of a function from vectors to vectors is the softmax:
\begin{equation*}
  \nfun{i}{softmax} A = \frac{\exp A}{\nsum{i} \exp A}
\end{equation*}

And it's also very handy to have a function that renames an index:
\begin{equation*}
% Don't delete this comment, which works around a bug in MathJax
[A]_{\name{bar}\rightarrow\name{baz}} = \nmatrix{foo}{baz}{
  3 & 1 & 4 \\
  1 & 5 & 9
}
\end{equation*}

Concatenation combines two vectors into one:
\begin{align*}
  A \nconcat{foo} B &= \nmatrix{foo}{bar}{
    3 & 1 & 4 \\
    1 & 5 & 9 \\
    2 & 7 & 1 \\
    8 & 2 & 8
  } \\
  A \nconcat{bar} B &= \nmatrix{foo}{bar}{
    3 & 1 & 4 & 2 & 7 & 1 \\
    1 & 5 & 9 & 8 & 2 & 8
  }
\end{align*}

\subsubsection{Matrices}

Finally, we briefly consider functions on matrices, for which you have to give \emph{two} index names (and the order in general matters). Let $A$ be a named tensor with shape $\{\name{i}:2,\name{j}:2,\name{k}:2\}$:
\begin{align*}
A_{\name{i}:1} &= \nmatrix{j}{k}{
  1 & 2 \\
  3 & 4
} \\
A_{\name{i}:2} &= \nmatrix{j}{k}{
  5 & 6 \\
  7 & 8
} \\
\det_{\name{j},\name{k}} A &= \nmatrix{}{i}{\det \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} & \det \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}} \\
\det_{\name{k},\name{j}} A &= \nmatrix{}{i}{\det \begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix} & \det \begin{bmatrix} 5 & 7 \\ 6 & 8 \end{bmatrix}} \\
\det_{\name{i},\name{j}} A &= \nmatrix{}{k}{\det \begin{bmatrix} 1 & 3 \\ 5 & 7 \end{bmatrix} & \det \begin{bmatrix} 2 & 4 \\ 6 & 8 \end{bmatrix}}
\end{align*}
For matrix inverses, there's no easy way to put a subscript under $\mathord\cdot^{-1}$, so we recommend writing $\mathop{\text{inv}}\displaylimits_{\name{i},\name{j}}$.

\section{Examples}
\label{sec:examples}

\subsection{Attention}

Let $d_k$ and $d_v$ be positive integers, and let $n$ and $n'$ be the input and output sequence length. Define a function
\begin{align*}
  \text{Att} &\colon \mathbb{R}^{\name{time'}:n',\name{key}:d_k} \times \mathbb{R}^{\name{time}:n,\name{key}:d_k} \times \mathbb{R}^{\name{time}:n,\name{val}:d_v} \rightarrow \mathbb{R}^{\name{time'}:n',\name{val}:d_v} \\
  \text{Att}(Q,K,V) &= \softmax_{\name{time}} \left( \frac{Q \ndot{key} K}{\sqrt{d_k}} \right) \ndot{time} V
\end{align*}

In self-attention, $Q$, $K$, and $V$ are all computed from the same sequence.  Let $\dmodel$ be a positive integer. The parameters are:
\begin{align*}
  W^Q &\in \mathbb{R}^{\name{emb}:\dmodel, \name{key}:d_k} \\
  W^K &\in \mathbb{R}^{\name{emb}:\dmodel, \name{key}:d_k} \\
  W^V &\in \mathbb{R}^{\name{emb}:\dmodel, \name{val}:d_v} \\
  W^O &\in \mathbb{R}^{\name{val}:d_v, \name{emb}:\dmodel}
\end{align*}
Then define
\begin{align*}
  \text{SelfAtt} &\colon \mathbb{R}^{\name{time}:n,\name{emb}:\dmodel} \rightarrow \mathbb{R}^{\name{time}:n,\name{emb}:\dmodel} \\
  \text{SelfAtt}(X; W^Q, W^K, W^V, W^O) &= W^O \ndot{val} \left[\text{Att}(Q, K, V)\right]_{\name{time'}\rightarrow\name{time}}
\end{align*}
where
\begin{align*}
  Q &= W^Q \ndot{emb} \left[X\right]_{\name{time}\rightarrow\name{time'}} \\
  K &= W^K \ndot{emb} X \\
  V &= W^V \ndot{emb} X.
\end{align*}

To change this to multi-head self-attention with $h$ attention heads, simply redefine 
\begin{align*}
  W^Q &\in \mathbb{R}^{\name{head}:h, \name{emb}:\dmodel, \name{key}:d_k} \\
  W^K &\in \mathbb{R}^{\name{head}:h, \name{emb}:\dmodel, \name{key}:d_k} \\
  W^V &\in \mathbb{R}^{\name{head}:h, \name{emb}:\dmodel, \name{val}:d_v} \\
  W^O &\in \mathbb{R}^{\name{head}:h, \name{val}:d_v, \name{emb}:\dmodel}
\end{align*}
and define
\begin{align*}
\text{MultiSelfAtt} &\colon \mathbb{R}^{\name{time}:n,\name{emb}:\dmodel} \rightarrow \mathbb{R}^{\name{time}:n,\name{emb}:\dmodel} \\
\text{MultiSelfAtt}(X; W^Q, W^K, W^V, W^O) &= \nsum{head} \text{SelfAtt}(X; W^Q, W^K, W^V, W^O).
\end{align*}

\subsection{RNN}
\label{sec:rnn}

As a second example, let's define a simple (Elman) RNN. Let $d$ be a positive integer.

\begin{align*}
x^{(t)} &\in \mathbb{R}^{\name{emb}: d} & t &= 1, \ldots, n \\
h^{(t)} &\in \mathbb{R}^{\name{state}: d} & t &= 0, \ldots, n\\
A &\in \mathbb{R}^{\name{state}: d, \name{state'}: d} \\
B &\in \mathbb{R}^{\name{emb}: d, \name{state'}: d} \\
c &\in \mathbb{R}^{\name{state'}: d} \\
h^{(t+1)} &= \left[ \tanh\left( A \ndot{state} h^{(t)} + B \ndot{emb} x^{(t)} + c \right) \right]_{\name{state'} \rightarrow \name{state}}
\end{align*}

The renaming is necessary because our notation doesn't provide a one-step way to apply a linear transformation ($A$) to one index and put the result in the same index. For possible solutions, see \S\ref{sec:duality}.

\subsection{Fully-Connected Layers}

Fully-connected layers are bit more verbose, but make more explicit which parameters connect which layers.

\begin{align*} 
V &\in \reals^{\name{output}:o, \name{hidden}:h} & c\in \reals^{\name{output}:o} \\
W &\in \reals^{{\name{hidden}:h, \name{in}:i}} & b \in \reals^{\name{hidden}:h} \\
X &\in \reals^{{\name{batch}:b, \name{in}:i}} \\
Y &= \sigma \left( W \ndot{in} X + b \right) \\
Z &= \sigma \left( V \ndot{hidden} Y + c \right)  
\end{align*}

\subsection{Deep Learning Norms}

These three functions are often informally described using the same
equation, but they each correspond to very different functions. They differ
by which axes are normalized.

\subsubsection*{Batch Norm}

\begin{align*} 
X &\in \reals^{{\name{batch}:b, \name{channel}:c, \name{hidden}:h}}\\
\gamma, \beta &\in \reals^{{\name{batch}:b}} \\
\text{batchnorm}(X; \gamma, \beta) &= \frac{X - \nfun{batch}{mean}(X)}{\sqrt{\nfun{batch}{var}(X)} + \epsilon } \odot \gamma + \beta
\end{align*}


\subsubsection*{Instance Norm}

\begin{align*} 
X &\in \reals^{{\name{batch}:b, \name{channel}:c, \name{hidden}:h}}\\
\gamma, \beta &\in \reals^{{\name{hidden}:h}} \\
\text{instancenorm}(X; \gamma, \beta) &= \frac{X - \nfun{hidden}{\text{mean}}(X)}{\sqrt{\nfun{hidden}{\text{var}}(X)} + \epsilon } \odot \gamma + \beta
\end{align*}

\subsubsection*{Layer Norm}

\begin{align*} 
X &\in \reals^{{\name{batch}:b, \name{channel}:c, \name{hidden}:h}} \\
\gamma, \beta &\in \reals^{{\name{channel}:c, \name{hidden}:h}} \\
\text{layernorm}(X; \gamma, \beta) &= \frac{X - \nfun{hidden,channel}{\text{mean}}(X)}{\sqrt{\nfun{hidden, channel}{\text{var}}(X)} + \epsilon } \odot \gamma + \beta 
\end{align*}


\subsection{Continuous Bag of Words}

A continuous bag-of-words model classifies by summing up the embeddings of a sequence of words $X$ and then projecting them to the space of classes. 

\begin{align*} 
X &\in \{0, 1\}^{{\name{time}:t, \name{vocab}:v}} & \nsum{vocab} X &= 1\\
E &\in \reals^{{\name{vocab}:v, \name{hidden}:h}}\\
W &\in \reals^{{\name{class}:c, \name{hidden}:h}}\\
\text{cbow}(X; E, W) &= \nfun{class}{softmax} (W \ndot{hidden} E \ndot{vocab} X)
\end{align*}
Here, the two contractions can be done in either order, so we leave the parentheses off.

\subsection{Bayes' Rule}

Named indices are very helpful for working with discrete random variables, because each random variable can be represented by an index with the same name. For instance, if $\name{A}$ and $\name{B}$ are random variables, we can treat $p(\name{B} \mid \name{A})$ and $p(\name{A})$ as tensors:
\begin{align*} 
p(\name{B} \mid \name{A}) &\in [0, 1]^{\name{A}:a, \name{B}:b} & \nsum{B} p(\name{B}\mid \name{A}) &= 1 \\
p(\name{A}) &\in [0, 1]^{\name{A}:a} & \nsum{A} p(\name{A}) &= 1
\end{align*}
Then Bayes' rule is just:
\begin{align*}
p(\name{A} \mid \name{B}) &= \frac{p(\name{B} \mid \name{A}) \odot p(\name{A})}{p(\name{B} \mid \name{A}) \ndot{A} p(\name{A})}.
\end{align*}

\subsection{Sudoku ILP}

Sudoku puzzles can be represented as  binary tiled tensors.
Given a grid we can check that it is valid by converting it to a grid of grids. 
Constraints then ensure that there is one digit per row, per column and per sub-box. 

\begin{align*} 
X &\in \{0, 1\}^{\name{row}: 9, \name{col}:9, \name{assign}:9}  \\
\text{check}(X) &=
\left(\nsum{assign} Y = 
\nsum{Row,row} Y = 
\nsum{Col, col} Y =  
\nsum{row, col} Y = 1 \right) \\
Y &\in \{0, 1\}^{\name{Row}: 3, \name{Col}:3, \name{row}: 3, \name{col}:3, \name{assign}:9}  \\
Y_{\name{Row}:r, \name{row}:r', \name{Col}:c, \name{col}:c'} &= X_{\name{row}:r\times 3 + r'-1, \name{col}:c\times3 + c'-1}, 
\end{align*} 


\subsection{Max Pooling}

Max pooling used in image recognition takes a similar form as the Sudoku example.

\begin{align*} 
X &\in \reals^{\name{height}: h, \name{width}:w} \\
\text{maxpool2d}(X, kh, kw) &=  \nfun{kh, kw}{max}\  U \\
U &\in \reals^{{\name{height}:h / kh,
\name{width}:w / kw, \name{kh}:kh, \name{kw}:kw}}  \\
U_{\name{height}:i, \name{width}:j, \name{kh}:di, \name{kw}:dj  } & = X_{\name{height}:i \times kh + di -1, \name{width}:j \times kw + dj -1}  
\end{align*}



\subsection{1D Convolution}

1D Convolution can be easily written by unrolling a tensor and then
applying a standard dot product.

\begin{align*} 
X &\in \reals^{{\name{channel}:c, \name{time}:t}}  \\
W &\in \reals^{{\name{out\_channel}:c', \name{channel}:c, \name{kw}:k}}  \\
\text{conv1d}(X, W) &= W \ndot{channel, kw} U \\
U &\in \reals^{{\name{channel}:c, \name{time}:t-k +1, \name{kw}:k}}  \\
U_{\name{time}:i, \name{kw:j}} & = X_{\name{time}:i+j - 1}  
\end{align*} 


\subsection{$K$-Means Clustering}

The following equations define one step of $k$-means clustering. Given a set of points $X$ and an initial set of cluster centers $C$,
\begin{align*} 
X &\in \reals^{\name{batch}:b, \name{dim}:k} \\
C &\in \reals^{\name{cluster}:c, \name{dim}:k}
\end{align*}
we compute cluster assignments
\begin{align*}
% Q &\in \{0, 1\}^{\name{batch}{b},\name{cluster}{c}} \\
Q &= \nfun{cluster}{argmin} \nfun{dim}{norm}(C-X) \\
  &= \lim_{\alpha \rightarrow -\infty} \nfun{cluster}{softmax} \left(\alpha \nfun{dim}{norm}(C-X)\right)
\end{align*}
then we recompute the cluster centers:
\begin{equation*}
C \leftarrow \nsum{batch} \frac{Q \odot X}{Q}.
\end{equation*}

\subsection{Beam Search}

Beam search is a commonly used approach for approximate discrete search. Here $H$ is the score of each element in the beam, $S$ is the state of each element in the beam, and $f$ is an update function that returns the score of each state transition. 
Beam step returns the new $H$ tensor. 

\begin{align*} 
H &\in \reals^{{ \name{batch}:b, \name{beam}:k}} \\
S &\in \{0, 1\}^{{ \name{batch}:b, \name{beam}:k, \name{state}:s}} & \nsum{state} S &= 1\\
f &\colon \{0, 1\}^{{\name{state}}} \rightarrow \reals^{{\name{state'}}} \\ 
\text{beamstep}(H, S) &=  \nfun{beam, state'}{maxk} \left( \nfun{state'}{softmax}(f(S)) \odot H  \right)
\end{align*} 

\subsection{Multivariate Normal}

In our notation, the application of a bilinear form is more verbose than the standard notation ($(X-\mu)^\top \Sigma^{-1} (X-\mu)$), but also makes it look more like a function of two arguments (and would generalize to three or more arguments).

\begin{align*} 
X &\in \reals^{\name{batch}:{b}, \name{d}:{k}}  \\
\mu &\in \reals^{{\name{d}:{k}}}  \\
\Sigma & \in   \reals^{{\name{d1}:{k}, \name{d2}:{k}}}  \\
{\cal N}(X; \mu, \Sigma) &= \frac{\displaystyle \exp\left(-\frac{1}{2}  \left(\nfun{d1, d2}{inv}(\Sigma) \ndot{d1} [X - \mu]_{\name{d} \rightarrow \name{d1}} \right) \ndot{d2} [X - \mu]_{\name{d} \rightarrow \name{d2}} \right) }{\sqrt{(2 \pi)^k \nfun{d1, d2}{det}(\Sigma)} }
\end{align*}


\subsection{Attention with Causal Masking}

When the Transformer is used for generation, it is necessary to have
an additional mask to ensure the model does not look at future words.
This can be included in the attention definition with clear names.

\begin{align*} 
Q &\in \reals^{\name{key}:d_v,\name{time'}:n}\\
K &\in \reals^{\name{head}:h,\name{key}:d_k, \name{time}:n}\\
V &\in \reals^{\name{head}:h, \name{val}:d_v, \name{time}:n}\\
\text{attention}(Q, K, V) &=  \nfun{time}{softmax} \left( \frac{Q \ndot{key} K }{\sqrt{d_k}} + M \right) \ndot{time} V \\
M & \in \reals^{\name{time}:n, \name{time'}:n} \\
M_{\name{time}:i, \name{time'}:j} & = \begin{cases}0 & i \leq j\\
-\infty & \text{otherwise} \end{cases} \\
\end{align*}


\subsection{Full Examples: Transformer and LeNet}

As further proof of concept, we have written the full models for Transformer (\url{https://namedtensor.github.io/transformer.html}) and LeNet (\url{https://namedtensor.github.io/convnet.html}). 

\section{Formal Definitions}
\label{sec:definitions}

\subsection{Named tuples}

A \emph{named tuple} is a set of pairs, written as $\tuple{\name{i}_1: x_1, \ldots, \name{i}_r: x_r}$, where $\name{i}_1, \ldots \name{i}_r$ are pairwise distinct \emph{names}. We write both names and variables ranging over names using sans-serif font.

If $t$ is a named tuple, we write $\tupledom{t}$ for the set $\{ \name{i} \mid \text{$(\name{i}:x) \in t$ for some $x$} \}$. If $\name{i} \in \tupledom{t}$, we write $\tupleproj{t}{\name{i}}$ for the unique $x$ such that $(\name{i}:x) \in t$. We write the empty named tuple as $\emptyset$.

We define a partial ordering $\sqsubseteq$ on named tuples: $t_1 \sqsubseteq t_2$ iff for all $\name{i}$, $x$, $(\name{i}:x) \in t_1$ implies $(\name{i}:x) \in t_2$. Then $t_1 \sqcap t_2$ is the greatest lower bound of $t_1$ and $t_2$, and $t_1 \sqcup t_2$ is their least upper bound.

A named tuple $\tuple{\name{i}_1: X_1, \ldots, \name{i}_r: X_r}$ where $X_1, \ldots, X_r$ are sets is called a \emph{shape}, which we will often use to yield a set of named tuples:
\begin{equation*}
\tupleshape \tuple{\name{i}_1: X_1, \ldots, \name{i}_r: X_r} = \left\{\tuple{\name{i}_1: x_1, \ldots, \name{i}_r: x_r} \mid x_1 \in X_1, \ldots, x_r \in X_r\right\}.
\end{equation*}

If $t \in \tupleshape \mathcal{T}$ and $\mathcal{S} \sqsubseteq \mathcal{T}$, then we write $\tuplerestrict{t}{\mathcal{S}}$ for the named tuple $\{(\name{i}: x) \in t \mid \name{i} \in \tupledom{S}\}$.

\subsection{Named tensors}

Let $[n] = \{1, \ldots, n\}$. We deal with shapes of the form $\tuple{\name{i}_1: [n_1], \ldots, \name{i}_r: [n_r]}$ so frequently that we define the shorthand $\tuple{\name{i}_1: n_1, \ldots, \name{i}_r: n_r}$.

Let $F$ be a field and let $\mathcal{T}$ be a shape. Then a \emph{named tensor over $F$ with shape $\mathcal{T}$} is a mapping from $\tupleshape \mathcal{T}$ to $F$. We write the set of all named tensors with shape $\mathcal{T}$ as $F^{\mathcal{T}}$. To avoid clutter, in place of $F^{\tuple{\name{i}_1: X_1, \ldots, \name{i}_r: X_r}}$, we usually write $F^{\name{i}_1: X_1, \ldots, \name{i}_r: X_r}$.

We don't make any distinction between a scalar (an element of $F$) and a named tensor with empty shape (an element of $F^\emptyset$).

If $A \in F^{\mathcal{T}}$, then we access an element of $A$ by applying it to a named tuple $t \in \tupleshape\mathcal{T}$; but we write this using the usual subscript notation: $A_t$ rather than $A(t)$. To avoid clutter, in place of $A_{\tuple{\name{i}_1: x_1, \ldots, \name{i}_r: x_r}}$, we usually write $A_{\name{i}_1: x_1, \ldots, \name{i}_r: x_r}$. When a named tensor is an expression like $(A+B)$, we surround it with square brackets like this: $[A+B]_{\name{i}_1: x_1, \ldots, \name{i}_r: x_r}$.

We also allow partial indices. Let $\mathcal{U}$ be a shape such that $\mathcal{U} = \mathcal{S} \sqcup \mathcal{T}$ and $\mathcal{S} \sqcap \mathcal{T} = \emptyset$.
If $A$ is a tensor with shape $\mathcal{S}$ and $s \in \tupleshape \mathcal{S}$, then we define $A_s$ to be the named tensor with shape $\mathcal{T}$ such that, for any $t \in \tupleshape \mathcal{T}$,
\begin{align*}
\left[A_s\right]_t &= A_{s \sqcup t}.
\end{align*}
(For the edge case $\mathcal{S} = \mathcal{U}$ and $\mathcal{T} = \emptyset$, our definitions for indexing and partial indexing coincide: one gives a scalar and the other gives a tensor with empty shape, but we don't distinguish between the two.)

\subsection{Extending functions to named tensors}
\label{sec:tensorfunctions}

In \S\ref{sec:intro}, we described several classes of functions that can be extended to named tensors. Here, we define how to do this for general functions.

Let $f \colon F^{\mathcal{S}} \rightarrow G^{\mathcal{T}}$ be a function from tensors to tensors. For any shape $\mathcal{U}$ such that $\mathcal{S} \sqcap \mathcal{U} = \emptyset$ and $\mathcal{T} \sqcap \mathcal{U} = \emptyset$, we can extend $f$ to:
\begin{align*}
f &: F^{\mathcal{S} \sqcup \mathcal{U}} \rightarrow G^{\mathcal{T} \sqcup \mathcal{U}} \\
[f(A)]_u &= f(A_u) \qquad \text{for all $u \in \tupleshape\mathcal{U}$.}
\end{align*}

If $f$ is a multary function, we can extend its arguments to larger shapes, and we don't have to extend all the arguments with the same names. We consider just the case of two arguments; three or more arguments are analogous. Let $f \colon F^{\mathcal{S}} \times G^{\mathcal{T}} \rightarrow H^{\mathcal{U}}$ be a binary function from tensors to tensors. For any shape $\mathcal{S'}$, $\mathcal{T'}$ such that $\mathcal{U'} = \mathcal{S'} \sqcup \mathcal{T'}$ exists, and 
\begin{align*}
\mathcal{S} \sqcap \mathcal{S'} &= \emptyset \\
\mathcal{T} \sqcap \mathcal{T'} &= \emptyset \\
\mathcal{U} \sqcap \mathcal{U'} &= \emptyset \end{align*}
we can extend $f$ to:
\begin{align*}
f &: F^{\mathcal{S} \sqcup \mathcal{S'}} \times G^{\mathcal{T} \sqcup \mathcal{T'}} \rightarrow H^{\mathcal{U} \sqcup \mathcal{U'}} \\
  [f(A,B)]_u &= f\left(A_{\tuplerestrict{u}{\mathcal{S'}}},B_{\tuplerestrict{u}{\mathcal{T'}}}\right) \qquad \text{for all $u \in \tupleshape\mathcal{U'}$.}
\end{align*}

All of the tensor operations described in \S\ref{sec:operations} can be defined in this way. For example, the contraction operator extends the following ``named dot-product'':
\begin{align*}
\ndot{i} &: F^{\name{i}:n} \times F^{\name{i}:n} \rightarrow F \\
A \ndot{i} B &= \sum_{i=1}^n A_{\name{i}:i} B_{\name{i}:i}.
\end{align*}

\section{Broadcasting and Indexing}
\label{broadcasting}

Broadcasting is extremely common in array-based programming, despite its being implicit and prone to errors.\footnote{This is, in my opinion, a glaring negligence of the \emph{Zen of Python}: ``Explicit is better than implicit''.}  Under the named tensor notations, we can develop an alternative formulation of broadcasting and indexing.

We say that named tensor shapes $\mathcal{S}$ is \emph{broadcastable} to $\mathcal{T}$ if  $\mathcal{S} \sqsubseteq \mathcal{T}$. This is compatible with the NumPy semantics except that they allow dimensions with size 1.

Consider the following operations between tensors $A \in \mathbb{R}^{\name{i}: m}$ and $B \in \mathbb{R}^{\name{j}: n}$. Sometimes we desire the following computation:
\begin{equation*}
  C_{\name{i}:i, \name{j}:j} = A_{\name{i}:i} + B_{\name{j}:j} \ .
\end{equation*}
Normally, we need to expand the dimensionalities of both vectors by 1, and then add them. A question here is: can we broadcast a tuple of named tensors to the same shape? In fact we can.
\begin{equation*}
  \mathrm{broadcast} : X_1^{\mathcal{S}_1} \times \cdots \times X_n^{\mathcal{S}_n} \to X_1^{\mathcal{T}} \times \cdots \times X_n^{\mathcal{T}}, \quad \mathcal{T} = \bigsqcup_{i=1}^n \mathcal{S}_i
\end{equation*}

if all axes with shared names match. Obviously, we can also extend this to operate on a \emph{named} tuple of named tensors:
\begin{equation*}
  \mathrm{broadcast} : \{ \name{i}_1: X_1^{\mathcal{S}_1}, \cdots, \name{i}_n: X_n^{\mathcal{S}_n} \} \to \{ \name{i}_1: X_1^{\mathcal{T}}, \cdots, \name{i}_n: X_n^{\mathcal{T}}\}. \quad \mathcal{T} = \bigsqcup_{i=1}^n \mathcal{S}_i
\end{equation*}

We'll use this to develop a formalism of advanced indexing under named tensor notations.

\paragraph{Advanced indexing} A common operation in array-based programming is \emph{advanced indexing}, whose semantics is vague\footnote{~\url{https://numpy.org/doc/stable/reference/arrays.indexing.html}.} and cannot be easily reasoned over. Given an \emph{unnamed} tensor $A \in X^{n_1 \times \cdots \times n_D}$ in NumPy (or PyTorch), $A$ can be indexed in the form of $A_{I^1, \cdots, I^D}$, where each \emph{indexer} $I^{d \in [D]}$ can be of the following three types:
\begin{itemize}
  \item An index $I^d \in [n_d]$;
  \item A subset of indices $I^d \subseteq [n_d]$ (a special case is ``\texttt{:}'', i.e. $I^d = [n_d]$);
  \item Another tensor with integer elements $I^d \in [n_d]^\mathcal{T}$ with shape $\mathcal{T}$.
\end{itemize}

What if $A$ is a named tensor of type $X^{\name{i}_1: n_1, \cdots, \name{i}_D: n_D}$? How can we extend the notion of advanced indexing to named version, i.e. $A_{\name{i}_1: I^1, \cdots, \name{i}_D: I^D}$?

We first start our discussion where all indexers $I^{d \in [D]}$ to the indexee $A \in X^{\mathcal{S}}$ are tensors in $[n_d]^{\mathcal{T}}$ with uniform shape $\mathcal{T} = \{\name{j}_1: m_1, \cdots, \name{j}_E : m_E\}$. The advanced indexing operation is defined as
\begin{align*}
  \mathrm{advIdx}&: X^{\mathcal{S}} \times \{ \name{i}_1: [n_1]^\mathcal{T} , \cdots, \name{i}_D: [n_D]^\mathcal{T}\} \to X^{\mathcal{T}} \\
  B  &= \mathrm{advIdx}(A, \{\name{i}_1: I^1, \cdots, \name{i}_D: I^D\}) \in X^{\mathcal{T}} \\
  B_{\name{j}_1:y_1, \cdots, \name{j}_E:y_E}  &= A_{\name{i}_1: I^1_{\name{j}_1:y_1, \cdots, \name{j}_E:y_E}, \cdots, \name{i}_D: I^D_{\name{j}_1:y_1, \cdots, \name{j}_E:y_E}} \\
\end{align*}

How do we fit the three types of indexers into this definition? We view each indexer $I^d$ as a named tensor with the following shape:
\begin{itemize}
  \item An index $I^d \in [n_d]$: We view $I^d \in [n_d]^\emptyset$;
  \item A subset of indices $I^d \subseteq [n_d]$: We view this as a 1-dimensional tensor with name $\name{i}_d$, hence $I^d \in [n_d]^{\name{i}_d: |I^d|}$;
  \item Another tensor with integer elements $I^d \in [n_d]^\mathcal{T}$ with shape $\mathcal{T}$.
\end{itemize}

Now the more lenient form of advanced indexing as in the semantics of NumPy can be described using the broadcast operator we elaborated above:
\begin{align*}
  \mathrm{advIdx}'&: X^{\mathcal{S}} \times \{ \name{i}_1: [n_1]^{\mathcal{T}_1} , \cdots , \name{i}_D: [n_D]^{\mathcal{T}_D} \} \to X^{\bigsqcup_{i=1}^D \mathcal{T}_i} \\
  B = A_{\name{i}_1: I^1, \cdots, \name{i}_D: I^D} &= \mathrm{advIdx}(A, \mathrm{broadcast}(\{ \name{i}_1: [n_1]^{\mathcal{T}_1} , \cdots , \name{i}_D: [n_D]^{\mathcal{T}_D} \}) \in X^{\mathcal{T}} \\
\end{align*}

Let's consider a concrete example in natural language processing. Given a batch of encoded sentences using a contextualizer like BERT, we get a tensor $X \in \mathbb{R}^{\name{batch}:B, \name{sentLen}: N, \name{emb}: E}$. For each sentence, we would like to take out the encodings of a specific span $[l_b, r_b)$ for each sentence $b \in [B]$ in the batch, resulting in a tensor $ Y \in \mathbb{R}^{\name{batch}:B, \name{spanLen}: M, \name{emb}: E}$.

We create a indexer for the $\name{sentLen}$ axis: $I_{\name{sentLen}} \in [N]^{\name{batch}:B, \name{spanLen}: M}$ that selects the desired encodings of tokens. The advanced indexing expression $X[:, I, :]$ can be viewed under our formulation as
\begin{equation*}
  \mathrm{advIdx}'(X, (I_{\name{batch}}, I_{\name{sentLen}}, I_{\name{emb}})) \ .
\end{equation*}

where
\begin{align*}
  I_{\name{batch}} &\in [B]^{\name{batch}:B} \ ; \\
  I_{\name{sentLen}} &\in [N]^{\name{batch}:B, \name{spanLen}: M} \ ; \\
  I_{\name{emb}} &\in [E]^{\name{emb}:E} \ . \\
\end{align*}

Based on our broadcast function, the three indexers when broadcast together has shape $\{\name{batch}:B, \name{spanLen}: M, \name{emb}: E\}$ just as we desired.

\section{Duality}
\label{sec:duality}

In applied linear algebra, we distinguish between column and row vectors; in pure linear algebra, vector spaces and dual vector spaces; in tensor algebra, contravariant and covariant indices; in quantum mechanics, bras and kets. Do we need something like this?

In \S\ref{sec:rnn} we saw that defining an RNN requires renaming of indices, because a linear transformation must map one index to another index; if we want to map an index to itself, we need to use renaming.

In this section, we describe three possible solutions to this problem, and welcome comments about which (if any) would be best.

\subsection{Contracting two names}

We define a version of the contraction operator that can contract two indices with different names. If $\name{i} \in \tupledom \mathcal{A}$ and $\name{j} \in \tupledom \mathcal{B}$ and $\tupleproj{\mathcal{A}}{\name{i}} = \tupleproj{\mathcal{B}}{\name{j}} = X$, then we define
\begin{equation*}
A \nndot{i}{j} B = \sum_{x \in X} A_{\name{i}: x} \, B_{\name{j}: x}
\end{equation*}

For example, the RNN would look like this.
\begin{align*}
x^{(t)} &\in \mathbb{R}^{\name{emb}: d} \\
h^{(t)} &\in \mathbb{R}^{\name{state}: d} \\
A &\in \mathbb{R}^{\name{state}: d, \name{state'}: d} \\
B &\in \mathbb{R}^{\name{emb}: d, \name{state}: d} \\
c &\in \mathbb{R}^{\name{state}: d} \\
h^{(t+1)} &= \tanh\left( A \nndot{state'}{state} h^{(t)} + B \ndot{emb} x^{(t)} + c \right)
\end{align*}

\subsection{Starred index names}

If $\name{i}$ is a name, we also allow a tensor to have an index $\name{i*}$ (alternatively: superscript $\name{i}$). Multiplication contracts starred indices in the left operand with non-starred indices in the right operand.
\begin{align*}
x^{(t)} &\in \mathbb{R}^{\name{emb}: d} \\
h^{(t)} &\in \mathbb{R}^{\name{state}: d} \\
A &\in \mathbb{R}^{\name{state*}: d, \name{state}: d} \\
B &\in \mathbb{R}^{\name{emb*}: d, \name{state}: d} \\
c &\in \mathbb{R}^{\name{state}: d} \\
h^{(t+1)} &= \tanh\left( A \ndot{state} h^{(t)} + B \ndot{emb} x^{(t)} + c \right) 
\end{align*}
In general, if $\name{i*} \in \tupledom \mathcal{A}$ and $\name{i} \in \tupledom \mathcal{B}$ and $\tupleproj{\mathcal{A}}{\name{i*}} = \tupleproj{\mathcal{B}}{\name{i}} = X$, then we define
\begin{equation*}
A \ndot{i} B = \sum_{x \in X} A_{\name{i*}: x} \, B_{\name{i}: x}
\end{equation*}

There are a few variants of this idea that have been floated:
\begin{enumerate}
\item $\ndot{}$ (no subscript) contracts every starred index in its left operand with every corresponding unstarred index in its right operand. Rejected.
\item $\ndot{i}$ contracts $\name{i}$ with $\name{i}$, and we need another notation like $\ndot{i(*)}$ or $\mathop\times\displaylimits_{\name{i}}$ for contracting $\name{i*}$ with $\name{i}$.
\item $\ndot{i}$ always contracts $\name{i*}$ with $\name{i}$; there's no way to contract $\name{i}$ with $\name{i}$.
\end{enumerate}


\subsection{Natural contraction}
Tensor contraction is variously termed as \texttt{einsum} (Einstein summation) in NumPy or \texttt{tensordot} in various integer-indexed tensor libraries. These notations require a list of index pairs to designate which axes to contract (e.g. \texttt{einsum([a, b], axes=[(1, 2), (2, 3)])}), which is pretty unreadable to a programmer. We consider a special case here.

Consider two tensors $A \in \mathbb{R}^\mathcal{S}$ and $B \in \mathbb{R}^\mathcal{T}$. We define the \emph{symmetric difference} of two shapes as
\begin{equation*}
\mathcal{S} ~\triangle~ \mathcal{T} = \left\{ (\name{i}: x) \mid \name{i} \in \tupledom{\mathcal{S}} ~\triangle~ \tupledom{\mathcal{T}} \right\} \ ,
\end{equation*}
where $\tuplerestrict{\mathcal{S}}{\mathcal{S} \sqcap \mathcal{T}} = \tuplerestrict{\mathcal{T}}{\mathcal{S} \sqcap \mathcal{T}} $, i.e., axes with the same name matches. For example, the shape of matrix multiplication can be expressed using the symmetric difference operator:
\begin{equation}
  \{ \name{i}: m, \name{j}: n \} ~\triangle~ \{ \name{j}: n, \name{k}: p \} = \{ \name{i}: m, \name{k}: p \} \ .
\end{equation}

Now we can introduce the notion of \emph{natural contraction} \citep{chen2017typesafe}. We define an operator 
\begin{equation*}
  \bowtie ~ : \mathbb{R}^{\mathcal{S}} \times \mathbb{R}^{\mathcal{T}} \to \mathcal{R}^{\mathcal{S}\triangle\mathcal{T}},
\end{equation*} 
where 
\begin{equation*}
  A \bowtie B \triangleq \sum_{x_1 \in X_1} \cdots \sum_{x_D \in X_D} A_{\name{i}_1: x_1, \cdots, \name{i}_D : x_D} \cdot B_{\name{i}_1: x_1, \cdots, \name{i}_D : x_D} \quad \forall (\name{i}_d: X_d) \in \mathcal{S} \triangle \mathcal{T} \ .
\end{equation*}
This says, given two tensors $A$ and $B$, contract all axes where the names match, and retain all other axes. Clearly, this subsumes various operations in linear algebra:
\begin{center}
  \begin{tabular}{ccc}
    Inner product & $\mathbf{a} \in \mathbb{R}^{\name{i}:n}$, $\mathbf{b} \in \mathbb{R}^{\name{i}:n} $ & $ \mathbf{a} \cdot \mathbf{b} =  \mathbf{a} \bowtie \mathbf{b} \in \mathbb{R}^\emptyset$ \\
    Matrix product & $\mathbf{A} \in \mathbb{R}^{\name{i}:m, \name{j}:n} $, $\mathbf{B} \in \mathbb{R}^{\name{j}:n, \name{k}:p} $ & $ \mathbf{A} \mathbf{B} =  \mathbf{A} \bowtie \mathbf{B} \in \mathbb{R}^{\name{i}:m, \name{k}:p}$ \\
    Tensor product & $\mathbf{A} \in \mathbb{R}^\mathcal{S} $, $\mathbf{B} \in \mathbb{R}^{\mathcal{T}} $ (if $\mathcal{S} \sqcap \mathcal{T} = \emptyset$) & $ \mathbf{A} \otimes \mathbf{B} =  \mathbf{A} \bowtie \mathbf{B} \in \mathbb{R}^{\mathcal{S} \sqcup \mathcal{T}}$  \\
  \end{tabular}
\end{center}

\paragraph{Belief propagation under named tensor notation} The natural contraction operator is a natural fit for describing the belief propagation algorithm in graphical models. Consider a function $g$ over a set of variables $\mathcal{X} = \{x_1, \cdots, x_N\}$ where $x_i \in X_i$:
\begin{equation*}
  g(\mathcal{X}) = \prod_{j=1}^M f_j(S_j)
\end{equation*}
where $S_j \subseteq \mathcal{X}$ is a subset of variables in $\mathcal{X}$. The corresponding factor graph $G = (\mathcal{X}, F, E)$ is a bipartite graph where
\begin{center}
  \begin{tabular}{cl}
    $\mathcal{X} = \{x_1, \cdots, x_N\}$ & Set of variables \\
    $F = \{f_1, \cdots, f_M\}$ & Set of potential functions \\
    $E = \{ (x_i, f_j) \mid x_i \in S_j \}$ & Set of undirected edges depending on factorization \\
  \end{tabular}
\end{center}

We assign each variable $x_i$ with a distinct \emph{name} $\name{i}_i$. The potential function $f_j$ over variables $S_j$ can now be viewed as a \emph{named tensor} with the following shape:
\begin{align*} 
  \mathcal{S}_j &= \{ (\name{i}_i : X_i) \} \quad \forall x_i \in S_j \ ; \\
  f_j &\in \mathbb{R}^{\mathcal{S}_j} \ .
\end{align*}

Additionally, messages being passed in the belief propagation algorithm, along with their computation in the sum-product algorithm, can also be described succinctly using named tensor notations:
\begin{center}
  \begin{tabular}{ccccc}
    $M_{x_i \to f_j}$ & = & $\displaystyle\bigodot_{f_k \in \mathrm{Nb}(x_i) \setminus \{f_j\}}{M_{f_k \to x_i}}$ & $\in$ & $\mathbb{R}^{\name{i}_i : X_i}$ \\
    \\
    $M_{f_j \to x_i}$ & = & $f_j \bowtie \displaystyle\bigbowtie_{x_k \in \mathrm{Nb}(f_j) \setminus 
    \{x_i\}}{M_{x_i \to f_j}}$ & $\in$ & $\mathbb{R}^{\name{i}_i : X_i}$ \\
  \end{tabular}
\end{center}

where $\mathrm{Nb}(\cdot)$ is the set of neighboring nodes in the factor graph $G$.

\subsection{Named and numbered indices}
\label{sec:tensorsoftensors}

We allow indices to have names that are natural numbers $1, 2, \ldots$, and we define ``numbering'' and ``naming'' operators:
\begin{center}
\begin{tabular}{cl}
$A_{\name{i}}$ & rename index $\name{i}$ to 1 \\
$A_{\name{i},\name{j}}$ & rename index $\name{i}$ to 1 and $\name{j}$ to 2 \\
$A_{\rightarrow\name{i}}$ & rename index 1 to $\name{i}$ \\
$A_{\rightarrow\name{i},\name{j}}$ & rename index 1 to $\name{i}$ and 2 to $\name{j}$
\end{tabular}
\end{center}
The numbering operators are only defined on tensors that have no numbered indices.

Then we adopt the convention that standard vector/matrix operations operate on the numbered indices. For example, vector dot-product always uses index 1 of both its operands, so that we can write
\begin{equation*}
C = A_{\name{i}} \cdot B_{\name{i}}
\end{equation*}
equivalent to $C = A \ndot{i} B$. 

Previously, we had to define a new version of every operation; most of the time, it looked similar to the standard version (e.g., $\max$ vs $\max_{\name{i}}$), but occasionally it looked quite different (e.g., matrix inversion). With numbered indices, we can use standard notation for everything.
(This also suggests a clean way to integrate code that uses named tensors with code that uses ordinary tensors.)

We also get the renaming operation for free: $A_{\name{i}\rightarrow\name{j}} = [A_{\name{i}}]_{\rightarrow\name{j}}$ renames index $\name{i}$ to $\name{j}$.

Finally, this notation alleviates the duality problem, as can be seen in the definition of a RNN:
\begin{align*}
x^{(t)} &\in \mathbb{R}^{\name{emb}: d} \\
h^{(t)} &\in \mathbb{R}^{\name{state}: d} \\
A &\in \mathbb{R}^{\name{state}: d, \name{state'}: d} \\
B &\in \mathbb{R}^{\name{state}: d, \name{emb}: d} \\
c &\in \mathbb{R}^{\name{state}: d} \\
h^{(t+1)}_{\name{state}} &= \tanh\left( A_{\name{state},\name{state'}} \, h^{(t)}_{\name{state}} + B_{\name{state},\name{emb}} \, x^{(t)}_{\name{emb}} + c_{\name{state}} \right)
\end{align*}
or equivalently,
\begin{equation*}
h^{(t+1)} = \tanh\left( A_{\name{state'}} \cdot h^{(t)}_{\name{state}} + B_{\name{emb}} \cdot x^{(t)}_{\name{emb}} + c \right)
\end{equation*}

Attention:
\begin{align*}
  \text{Att} &\colon \mathbb{R}^{\name{time'}:n',\name{key}:d_k} \times \mathbb{R}^{\name{time}:n,\name{key}:d_k} \times \mathbb{R}^{\name{time}:n,\name{val}:d_v} \rightarrow \mathbb{R}^{\name{time'}:n',\name{val}:d_v} \\
  \text{Att}(Q,K,V) &= \softmax \left[ \frac{Q_{\name{key}} \cdot K_\name{key}}{\sqrt{d_k}} \right]_{\name{time}} \cdot V_{\name{time}}
\end{align*}

Multivariate normal distribution:
\begin{align*} 
X &\in \mathbb{R}^{\name{batch}:{b}, \name{d}:{k}}  \\
\mu &\in \mathbb{R}^{{\name{d}:{k}}}  \\
\Sigma & \in   \mathbb{R}^{{\name{d}:{k}, \name{d'}:{k}}}  \\
{\cal N}(X; \mu, \Sigma) &= \frac{\displaystyle \exp\left(-\tfrac{1}{2} [X - \mu]_{\name{d}}^\top \, \Sigma_{\name{d},\name{d'}}^{-1} \, [X - \mu]_{\name{d}} \right) }{\sqrt{(2 \pi)^k \, \mathop{\text{det}} \Sigma_{\name{d},\name{d'}}} }
\end{align*}

Because this notation can be a little more verbose (often requiring you to write index names twice), we'd keep around the notation $A \ndot{i} B$ as a shorthand for $A_{\name{i}} \cdot B_{\name{i}}$. We'd also keep named reductions, or at least $\nfun{i}{softmax}$.

\section*{Acknowledgements}

Thanks to Ekin Aky\"{u}rek, Colin McDonald, Chung-chieh Shan, and Nishant Sinha for their input to this document (or the ideas in it).


\bibliographystyle{acl_natbib}
\bibliography{references}

\end{document}
